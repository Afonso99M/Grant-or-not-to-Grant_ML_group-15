{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn import base\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import lightgbm as lgb\n",
    "import itertools\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from collections import defaultdict \n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from utils import *\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "\n",
    "from collections import Counter\n",
    "import inspect\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# imports dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_new_feats.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = [[f\"target_{i}\" for i in range(1, 9)] + [\"Claim Injury Type\"] + [\"WCB Decision\"] + [\"Agreement Reached\"] + [\"Claim Injury Type_encoded\"]]\n",
    "target = [item for sublist in target for item in sublist]\n",
    "target\n",
    "\n",
    "binary_target = [f\"target_{i}\" for i in range(1, 9)]\n",
    "\n",
    "original_target  = [col for col in target if col not in binary_target]\n",
    "\n",
    "ordinal_target = [\"Claim Injury Type_encoded\"]\n",
    "\n",
    "features = [feat for feat in df.columns if feat not in target]\n",
    "\n",
    "features = [feat for feat in features if df[feat].dtype != \"datetime64[ns]\"]\n",
    "\n",
    "num_feats = [feat for feat in features if df[feat].dtype != \"object\"]\n",
    "\n",
    "cat_feats = [feat for feat in features if df[feat].dtype == \"object\"]\n",
    "cat_feats_index = [features.index(feat) for feat in cat_feats]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def num_imputing(X_train, X_val):\n",
    "    feats_imput_max = [\"C2_Accident_gap_weeks\", \"C3_Accident_gap_weeks\", \"Accident Date_assembly_gap_days\", \"Hearing_C3 gap_months\", \"Hearing_C2 gap_months\", \"Hearing_assembly_gap_months\", \"Days to First Hearing\"]\n",
    "\n",
    "    feat_imput_min = [\"C3-C2_gap_days\"]\n",
    "    \n",
    "    for feat in X_train.columns:\n",
    "        if X_train[feat].isna().sum() > 0 or X_val[feat].isna().sum() > 0:\n",
    "            if feat in feats_imput_max:\n",
    "                X_train[feat] = X_train[feat].fillna(X_train[feat].max())\n",
    "                X_val[feat] = X_val[feat].fillna(X_train[feat].max())\n",
    "            elif feat in feat_imput_min:\n",
    "                X_train[feat] = X_train[feat].fillna(X_train[feat].min())\n",
    "                X_val[feat] = X_val[feat].fillna(X_train[feat].min())\n",
    "            else:\n",
    "                X_train[feat] = X_train[feat].fillna(X_train[feat].mean())\n",
    "                X_val[feat] = X_val[feat].fillna(X_train[feat].mean())\n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_thresholds(y_true, probabilities):\n",
    "    best_thresholds = []\n",
    "    for i in range(probabilities.shape[1]):  # Loop over each class\n",
    "        precision, recall, thresholds = precision_recall_curve((y_true == i).astype(int), probabilities[:, i])\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "        best_thresholds.append(thresholds[np.argmax(f1_scores)])  # Store best threshold\n",
    "    return best_thresholds\n",
    "\n",
    "def predict_with_thresholds(probabilities, thresholds):\n",
    "    weighted_probs = probabilities / np.array(thresholds)  \n",
    "    predictions = np.argmax(weighted_probs, axis=1)  \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def model_predictions_global(X_train, y_train, X_val, y_val, model, clf_name, specific_target, cv_i, params):\n",
    "    \n",
    "    train_proba = model.predict_proba(X_train)\n",
    "    val_proba = model.predict_proba(X_val)\n",
    "    \n",
    "    best_thresholds = optimize_thresholds(y_val, val_proba)\n",
    "    \n",
    "    train_predictions = predict_with_thresholds(train_proba, best_thresholds)\n",
    "    val_predictions = predict_with_thresholds(val_proba, best_thresholds)\n",
    "    \n",
    "    \n",
    "    f1_score_train = f1_score(y_train, train_predictions, average=\"macro\")\n",
    "    f1_score_val = f1_score(y_val, val_predictions, average=\"macro\")\n",
    "    \n",
    "    print(f\"{clf_name}...\")\n",
    "    print(f\"Params: {params}\")\n",
    "    print(f\"Train F1-score: {round(f1_score_train, 3)}\")\n",
    "    print(f\"Thresholds: {best_thresholds}\")\n",
    "    print(f\"Validation F1-score: {round(f1_score_val, 3)}\")\n",
    "    print(classification_report(y_val, val_predictions))\n",
    "    \n",
    "    return params, f1_score_train, f1_score_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def frequency_encoding(train_df, val_df, column):\n",
    "    \"\"\"\n",
    "    Apply frequency encoding on the training set and use the same encoding to impute the validation set.\n",
    "    \n",
    "    Parameters:\n",
    "    train_df (pd.DataFrame): Training dataset.\n",
    "    val_df (pd.DataFrame): Validation dataset.\n",
    "    column (str): Column to encode.\n",
    "    \n",
    "    Returns:\n",
    "    train_encoded (pd.DataFrame): Encoded training set.\n",
    "    val_encoded (pd.DataFrame): Encoded validation set.\n",
    "    freq_map (dict): Mapping of frequency counts for the column.\n",
    "    \"\"\"\n",
    "    # Compute frequency encoding for the training set\n",
    "    freq_map = train_df[column].value_counts(normalize=True)  # Relative frequency\n",
    "    train_df[f\"{column}_freq\"] = train_df[column].map(freq_map)\n",
    "\n",
    "    # Impute frequency encoding on the validation set using the same mapping\n",
    "    val_df[f\"{column}_freq\"] = val_df[column].map(freq_map)\n",
    "\n",
    "    # Handle unseen categories in validation by imputing 0 frequency\n",
    "    val_df[f\"{column}_freq\"] = val_df[f\"{column}_freq\"].fillna(0)\n",
    "    \n",
    "    train_df = train_df.drop(columns=[column])\n",
    "    val_df = val_df.drop(columns=[column])\n",
    "\n",
    "    # Return encoded datasets and frequency map\n",
    "    return train_df, val_df, freq_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def target_guided_ordinal_encoding(X_train, X_val, categorical_column, target_column, y_train, i):\n",
    "    # Combine X_train with y_train temporarily to calculate means\n",
    "    X_train_encoded = X_train.copy()\n",
    "    X_val_encoded = X_val.copy()\n",
    "    X_train_encoded[target_column] = y_train\n",
    "\n",
    "    means = X_train_encoded.groupby(categorical_column)[target_column].mean()\n",
    "    #print(means)\n",
    "\n",
    "    sorted_means = means.sort_values(by=target_column)\n",
    "    #print(sorted_means)\n",
    "    # if i == 1:\n",
    "    #     print(f\"Showing sorted means for {categorical_column}\")\n",
    "    #     lst_names = sorted_means.index.tolist()\n",
    "    #     lst_values = sorted_means.values.tolist()\n",
    "    #     dict_final = dict(zip(lst_names, lst_values))\n",
    "    #     print(dict_final)\n",
    "    \n",
    "    ordinal_mapping = {category: rank for rank, category in enumerate(sorted_means.index, start=1)}\n",
    "    # if i == 1:\n",
    "    #     print(f\"Showing ordinal mapping for {categorical_column}\")\n",
    "    #     print(ordinal_mapping)\n",
    "    #     print(\"--------------------------------\")\n",
    "        \n",
    "    X_train_encoded[f\"{categorical_column}_encoded\"] = X_train_encoded[categorical_column].map(ordinal_mapping)\n",
    "    X_val_encoded[f\"{categorical_column}_encoded\"] = X_val_encoded[categorical_column].map(ordinal_mapping)\n",
    "\n",
    "    #X_train_encoded = X_train_encoded.drop(columns=[categorical_column])\n",
    "    X_train_encoded = X_train_encoded.drop(columns=[target_column[0]])\n",
    "    #X_val_encoded = X_val_encoded.drop(columns=[categorical_column])\n",
    "    X_train_encoded = X_train_encoded.fillna(1)\n",
    "    X_val_encoded = X_val_encoded.fillna(1)\n",
    "\n",
    "    return X_train_encoded, X_val_encoded, ordinal_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "selected_features=['Attorney/Representative',\n",
    " 'IME-4 Count',\n",
    " 'Accident Date_year',\n",
    " 'Accident Date_assembly_gap_days',\n",
    " 'C3-C2_gap_days',\n",
    " 'C2_missing',\n",
    " 'C3_missing',\n",
    " 'C3_Accident_gap_weeks',\n",
    " 'Hearing_C3 gap_months',\n",
    " 'Hearing_C2 gap_months',\n",
    " 'Days to Assembly',\n",
    " 'Days to First Hearing',\n",
    " 'Average Weekly Wage_log',\n",
    " 'Carrier Name_encoded',\n",
    " 'Carrier Type_encoded',\n",
    " 'Industry Code Description_encoded',\n",
    " 'WCIO Cause of Injury Description_encoded',\n",
    " 'WCIO Nature of Injury Description_encoded',\n",
    " 'WCIO Part Of Body Description_encoded',\n",
    " 'Carrier Name_freq',\n",
    " 'Carrier Type_freq',\n",
    " 'Industry Code Description_freq',\n",
    " 'WCIO Nature of Injury Description_freq',\n",
    " 'WCIO Part Of Body Description_freq']\n",
    "\n",
    "naive_features = [feat.replace(\"_encoded\", \"\") for feat in selected_features]\n",
    "naive_features = [feat.replace(f\"_freq\", \"\") for feat in naive_features]\n",
    "naive_features = set(naive_features)\n",
    "naive_features = list(naive_features)\n",
    "\n",
    "cat_feats = [feat for feat in naive_features if feat in cat_feats]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df[naive_features], df[ordinal_target], test_size=0.25, random_state=42, stratify=df[ordinal_target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinal encoding...\n",
      "Frequency encoding...\n",
      "Impuiting missing values...\n",
      "Scaling numericals ...\n",
      "Pre-fitting models...\n",
      "Iteration 1, loss = 0.65646500\n",
      "Iteration 2, loss = 0.62093225\n",
      "Iteration 3, loss = 0.61367077\n",
      "Iteration 4, loss = 0.60988526\n",
      "Iteration 5, loss = 0.60731536\n",
      "Iteration 6, loss = 0.60543196\n",
      "Iteration 7, loss = 0.60407227\n",
      "Iteration 8, loss = 0.60350148\n",
      "Iteration 9, loss = 0.60279669\n",
      "Iteration 10, loss = 0.60254505\n",
      "Iteration 11, loss = 0.60209650\n",
      "Iteration 12, loss = 0.60168521\n",
      "Iteration 13, loss = 0.60089948\n",
      "Iteration 14, loss = 0.60133125\n",
      "Iteration 15, loss = 0.60030259\n",
      "Iteration 16, loss = 0.60015537\n",
      "Iteration 17, loss = 0.60046001\n",
      "Iteration 18, loss = 0.60015482\n",
      "Iteration 19, loss = 0.60010909\n",
      "Iteration 20, loss = 0.59929760\n",
      "Iteration 21, loss = 0.60011643\n",
      "Iteration 22, loss = 0.59978173\n",
      "Iteration 23, loss = 0.59934325\n",
      "Iteration 24, loss = 0.59931830\n",
      "Iteration 25, loss = 0.59941536\n",
      "Iteration 26, loss = 0.59964759\n",
      "Iteration 27, loss = 0.59927294\n",
      "Iteration 28, loss = 0.59884341\n",
      "Iteration 29, loss = 0.59931767\n",
      "Iteration 30, loss = 0.59866886\n",
      "Iteration 31, loss = 0.59893333\n",
      "Iteration 32, loss = 0.59945058\n",
      "Iteration 33, loss = 0.59930027\n",
      "Iteration 34, loss = 0.59863268\n",
      "Iteration 35, loss = 0.59898196\n",
      "Iteration 36, loss = 0.59935901\n",
      "Iteration 37, loss = 0.59906894\n",
      "Iteration 38, loss = 0.59888914\n",
      "Iteration 39, loss = 0.59899783\n",
      "Iteration 40, loss = 0.59913227\n",
      "Iteration 41, loss = 0.59872630\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0:\tlearn: 0.6715310\ttotal: 2.04s\tremaining: 33m 56s\n",
      "100:\tlearn: 0.2075468\ttotal: 3m 9s\tremaining: 28m 2s\n",
      "200:\tlearn: 0.1834566\ttotal: 6m 19s\tremaining: 25m 9s\n",
      "300:\tlearn: 0.1771620\ttotal: 9m 39s\tremaining: 22m 25s\n",
      "400:\tlearn: 0.1736516\ttotal: 12m 58s\tremaining: 19m 23s\n",
      "500:\tlearn: 0.1711641\ttotal: 16m 8s\tremaining: 16m 4s\n",
      "600:\tlearn: 0.1693719\ttotal: 18m 41s\tremaining: 12m 24s\n",
      "700:\tlearn: 0.1679694\ttotal: 21m 13s\tremaining: 9m 3s\n",
      "800:\tlearn: 0.1667604\ttotal: 24m 47s\tremaining: 6m 9s\n",
      "900:\tlearn: 0.1658017\ttotal: 27m 22s\tremaining: 3m\n",
      "999:\tlearn: 0.1649023\ttotal: 30m 19s\tremaining: 0us\n",
      "Testing {'C': 1, 'multi_class': 'ovr', 'class_weight': 'balanced'} for metalearner...\n",
      "Stacking nn and catboost...\n",
      "stacking_nn_catboost...\n",
      "Params: None\n",
      "Train F1-score: 0.456\n",
      "Thresholds: [0.5171598329160946, 0.2330517719602787, 0.31353765966215663, 0.23861446638493874, 0.3079532559807111, 0.3404045705965176, 0.24311001696212117, 0.424670689161381]\n",
      "Validation F1-score: 0.432\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.52      0.59      3119\n",
      "         1.0       0.87      0.95      0.91     72770\n",
      "         2.0       0.40      0.15      0.22     17227\n",
      "         3.0       0.73      0.85      0.79     37127\n",
      "         4.0       0.67      0.64      0.66     12070\n",
      "         5.0       0.08      0.01      0.01      1053\n",
      "         6.0       0.02      0.25      0.03        24\n",
      "         7.0       0.23      0.28      0.25       117\n",
      "\n",
      "    accuracy                           0.79    143507\n",
      "   macro avg       0.46      0.46      0.43    143507\n",
      "weighted avg       0.75      0.79      0.76    143507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "stacking_model_params = {\"C\": [1], \"multi_class\": [\"ovr\"], \"class_weight\": [\"balanced\"]}\n",
    "stacking_results = []\n",
    "\n",
    "\n",
    "X_train_encoded = X_train.copy()\n",
    "X_val_encoded = X_val.copy()\n",
    "\n",
    "print(f\"Ordinal encoding...\")\n",
    "X_train_encoded = X_train.copy()\n",
    "X_val_encoded = X_val.copy()\n",
    "for cat in cat_feats:\n",
    "    X_train_encoded, X_val_encoded, ordinal_mapping = target_guided_ordinal_encoding(X_train_encoded, X_val_encoded, cat, ordinal_target, y_train, 1)\n",
    "\n",
    "print(f\"Frequency encoding...\")\n",
    "for cat in cat_feats:\n",
    "    X_train_encoded, X_val_encoded, freq_map = frequency_encoding(X_train_encoded, X_val_encoded, cat)\n",
    "\n",
    "X_train_encoded  = X_train_encoded[selected_features]\n",
    "X_val_encoded = X_val_encoded[selected_features]\n",
    "\n",
    "print(f\"Impuiting missing values...\")\n",
    "X_train_imputed, X_val_imputed = num_imputing(X_train_encoded, X_val_encoded)\n",
    "\n",
    "print(f\"Scaling numericals ...\")\n",
    "X_train_imputed, X_val_imputed = num_scaling(X_train_imputed, X_val_imputed)\n",
    "\n",
    "\n",
    "# ----------------- Stacking single models \n",
    "print(f\"Pre-fitting models...\")\n",
    "estimators = [(\"nn\", MLPClassifier(random_state=42, verbose=10, hidden_layer_sizes = (25,8), learning_rate_init=0.01).fit(X_train_imputed, y_train)),\n",
    "                (\"catboost\", CatBoostClassifier(random_state=42, verbose=100, iterations=1000, depth=6, boosting_type='Ordered', auto_class_weights='SqrtBalanced', loss_function=\"MultiClassOneVsAll\").fit(X_train_imputed, y_train))]\n",
    "\n",
    "\n",
    "metalearner_best_params = {}\n",
    "keys, values = zip(*stacking_model_params.items())\n",
    "for combination in itertools.product(*values):\n",
    "    params = dict(zip(keys, combination))\n",
    "    \n",
    "    print(f\"Testing {params} for metalearner...\")\n",
    "    print(f\"Stacking {estimators[0][0]} and {estimators[1][0]}...\")\n",
    "    st = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(random_state=42, ** params), cv=\"prefit\").fit(X_train_imputed, y_train)\n",
    "\n",
    "    params, f1_score_train, f1_score_val = model_predictions_global(X_train_imputed, y_train, X_val_imputed, y_val, st, f\"stacking_{estimators[0][0]}_{estimators[1][0]}\", \"\", 1, None)\n",
    "    \n",
    "    stacking_results.append({\"params\": params, \"f1_score_train\": f1_score_train, \"f1_score_val\": f1_score_val})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_results1 = stacking_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': None,\n",
       "  'f1_score_train': 0.45606447250558896,\n",
       "  'f1_score_val': 0.43191298089588276}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinal encoding...\n",
      "Frequency encoding...\n",
      "Impuiting missing values...\n",
      "Scaling numericals ...\n",
      "Pre-fitting models...\n",
      "0:\tlearn: 0.6715310\ttotal: 4.58s\tremaining: 1h 16m 17s\n",
      "100:\tlearn: 0.2075468\ttotal: 2m 50s\tremaining: 25m 14s\n",
      "200:\tlearn: 0.1834566\ttotal: 5m 29s\tremaining: 21m 51s\n",
      "300:\tlearn: 0.1771620\ttotal: 8m 13s\tremaining: 19m 4s\n",
      "400:\tlearn: 0.1736516\ttotal: 10m 34s\tremaining: 15m 47s\n",
      "500:\tlearn: 0.1711641\ttotal: 12m 49s\tremaining: 12m 46s\n",
      "600:\tlearn: 0.1693719\ttotal: 15m 12s\tremaining: 10m 5s\n",
      "700:\tlearn: 0.1679694\ttotal: 17m 27s\tremaining: 7m 26s\n",
      "800:\tlearn: 0.1667604\ttotal: 19m 40s\tremaining: 4m 53s\n",
      "900:\tlearn: 0.1658017\ttotal: 22m 5s\tremaining: 2m 25s\n",
      "999:\tlearn: 0.1649023\ttotal: 24m 17s\tremaining: 0us\n",
      "Testing {'C': 1, 'multi_class': 'ovr', 'class_weight': 'balanced'} for metalearner...\n",
      "Stacking NB and catboost...\n",
      "stacking_NB_catboost...\n",
      "Params: None\n",
      "Train F1-score: 0.455\n",
      "Thresholds: [0.526392525527319, 0.23193046277441545, 0.3170130517148906, 0.23098867317845276, 0.3126139754720139, 0.34846914616394437, 0.2369777156525646, 0.3346253551689305]\n",
      "Validation F1-score: 0.434\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.53      0.60      3119\n",
      "         1.0       0.87      0.95      0.91     72770\n",
      "         2.0       0.41      0.14      0.20     17227\n",
      "         3.0       0.73      0.87      0.79     37127\n",
      "         4.0       0.69      0.62      0.65     12070\n",
      "         5.0       0.16      0.01      0.01      1053\n",
      "         6.0       0.02      0.21      0.03        24\n",
      "         7.0       0.19      0.44      0.27       117\n",
      "\n",
      "    accuracy                           0.79    143507\n",
      "   macro avg       0.47      0.47      0.43    143507\n",
      "weighted avg       0.75      0.79      0.76    143507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stacking_model_params = {\"C\": [1], \"multi_class\": [\"ovr\"], \"class_weight\": [\"balanced\"]}\n",
    "stacking_results = []\n",
    "\n",
    "X_train_encoded = X_train.copy()\n",
    "X_val_encoded = X_val.copy()\n",
    "\n",
    "print(f\"Ordinal encoding...\")\n",
    "X_train_encoded = X_train.copy()\n",
    "X_val_encoded = X_val.copy()\n",
    "for cat in cat_feats:\n",
    "    X_train_encoded, X_val_encoded, ordinal_mapping = target_guided_ordinal_encoding(X_train_encoded, X_val_encoded, cat, ordinal_target, y_train, 1)\n",
    "\n",
    "print(f\"Frequency encoding...\")\n",
    "for cat in cat_feats:\n",
    "    X_train_encoded, X_val_encoded, freq_map = frequency_encoding(X_train_encoded, X_val_encoded, cat)\n",
    "\n",
    "X_train_encoded  = X_train_encoded[selected_features]\n",
    "X_val_encoded = X_val_encoded[selected_features]\n",
    "\n",
    "print(f\"Impuiting missing values...\")\n",
    "X_train_imputed, X_val_imputed = num_imputing(X_train_encoded, X_val_encoded)\n",
    "\n",
    "print(f\"Scaling numericals ...\")\n",
    "X_train_imputed, X_val_imputed = num_scaling(X_train_imputed, X_val_imputed)\n",
    "\n",
    "\n",
    "# ----------------- Stacking single models \n",
    "print(f\"Pre-fitting models...\")\n",
    "estimators = [(\"NB\", GaussianNB(var_smoothing=0.1).fit(X_train_imputed, y_train)),\n",
    "                (\"catboost\", CatBoostClassifier(random_state=42, verbose=100, iterations=1000, depth=6, boosting_type='Ordered', auto_class_weights='SqrtBalanced', loss_function=\"MultiClassOneVsAll\").fit(X_train_imputed, y_train))]\n",
    "\n",
    "metalearner_best_params = {}\n",
    "keys, values = zip(*stacking_model_params.items())\n",
    "for combination in itertools.product(*values):\n",
    "    params = dict(zip(keys, combination))\n",
    "    print(f\"Testing {params} for metalearner...\")\n",
    "        \n",
    "    print(f\"Stacking {estimators[0][0]} and {estimators[1][0]}...\")\n",
    "    st = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(random_state=42, ** params), cv=\"prefit\").fit(X_train_imputed, y_train)\n",
    "\n",
    "    params, f1_score_train, f1_score_val = model_predictions_global(X_train_imputed, y_train, X_val_imputed, y_val, st, f\"stacking_{estimators[0][0]}_{estimators[1][0]}\", \"\", 1, None)\n",
    "    \n",
    "    stacking_results.append({\"params\": params, \"f1_score_train\": f1_score_train, \"f1_score_val\": f1_score_val})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': None,\n",
       "  'f1_score_train': 0.4551124268119682,\n",
       "  'f1_score_val': 0.433842239269234}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_results2 = stacking_results\n",
    "stacking_results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
