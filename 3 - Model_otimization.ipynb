{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn import base\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import lightgbm as lgb\n",
    "import itertools\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from collections import defaultdict \n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from utils import *\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "\n",
    "from collections import Counter\n",
    "import inspect\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "# imports dataset\n",
    "\n",
    "df = pd.read_csv(\"train_new_feats.csv\")\n",
    "\n",
    "#agreement_reached_predicted = pd.read_csv(\"agreement_predictions.csv\")\n",
    "\n",
    "#df = pd.concat([df, agreement_reached_predicted], axis=1)\n",
    "\n",
    "df.columns\n",
    "\n",
    "#df.set_index(\"Claim Identifier\", inplace=True)\n",
    "\n",
    "# ------------ Subsets of features  \n",
    "\n",
    "df.columns\n",
    "\n",
    "target = [[f\"target_{i}\" for i in range(1, 9)] + [\"Claim Injury Type\"] + [\"WCB Decision\"] + [\"Agreement Reached\"] + [\"Claim Injury Type_encoded\"]]\n",
    "target = [item for sublist in target for item in sublist]\n",
    "target\n",
    "\n",
    "binary_target = [f\"target_{i}\" for i in range(1, 9)]\n",
    "\n",
    "original_target  = [col for col in target if col not in binary_target]\n",
    "\n",
    "ordinal_target = [\"Claim Injury Type_encoded\"]\n",
    "\n",
    "features = [feat for feat in df.columns if feat not in target]\n",
    "\n",
    "features = [feat for feat in features if df[feat].dtype != \"datetime64[ns]\"]\n",
    "\n",
    "num_feats = [feat for feat in features if df[feat].dtype != \"object\"]\n",
    "\n",
    "cat_feats = [feat for feat in features if df[feat].dtype == \"object\"]\n",
    "cat_feats_index = [features.index(feat) for feat in cat_feats]\n",
    "\n",
    "\n",
    "df[features+binary_target].columns\n",
    "\n",
    "def num_imputing(X_train, X_val):\n",
    "    feats_imput_max = [\"C2_Accident_gap_weeks\", \"C3_Accident_gap_weeks\", \"Accident Date_assembly_gap_days\", \"Hearing_C3 gap_months\", \"Hearing_C2 gap_months\", \"Hearing_assembly_gap_months\", \"Days to First Hearing\"]\n",
    "\n",
    "    feat_imput_min = [\"C3-C2_gap_days\"]\n",
    "    \n",
    "    for feat in X_train.columns:\n",
    "        if X_train[feat].isna().sum() > 0 or X_val[feat].isna().sum() > 0:\n",
    "            if feat in feats_imput_max:\n",
    "                X_train[feat] = X_train[feat].fillna(X_train[feat].max())\n",
    "                X_val[feat] = X_val[feat].fillna(X_train[feat].max())\n",
    "            elif feat in feat_imput_min:\n",
    "                X_train[feat] = X_train[feat].fillna(X_train[feat].min())\n",
    "                X_val[feat] = X_val[feat].fillna(X_train[feat].min())\n",
    "            else:\n",
    "                X_train[feat] = X_train[feat].fillna(X_train[feat].mean())\n",
    "                X_val[feat] = X_val[feat].fillna(X_train[feat].mean())\n",
    "    return X_train, X_val\n",
    "\n",
    "def frequency_encoding(train_df, val_df, column):\n",
    "    \"\"\"\n",
    "    Apply frequency encoding on the training set and use the same encoding to impute the validation set.\n",
    "    \n",
    "    Parameters:\n",
    "    train_df (pd.DataFrame): Training dataset.\n",
    "    val_df (pd.DataFrame): Validation dataset.\n",
    "    column (str): Column to encode.\n",
    "    \n",
    "    Returns:\n",
    "    train_encoded (pd.DataFrame): Encoded training set.\n",
    "    val_encoded (pd.DataFrame): Encoded validation set.\n",
    "    freq_map (dict): Mapping of frequency counts for the column.\n",
    "    \"\"\"\n",
    "    # Compute frequency encoding for the training set\n",
    "    freq_map = train_df[column].value_counts(normalize=True)  # Relative frequency\n",
    "    train_df[f\"{column}_freq\"] = train_df[column].map(freq_map)\n",
    "\n",
    "    # Impute frequency encoding on the validation set using the same mapping\n",
    "    val_df[f\"{column}_freq\"] = val_df[column].map(freq_map)\n",
    "\n",
    "    # Handle unseen categories in validation by imputing 0 frequency\n",
    "    val_df[f\"{column}_freq\"] = val_df[f\"{column}_freq\"].fillna(0)\n",
    "    \n",
    "    train_df = train_df.drop(columns=[column])\n",
    "    val_df = val_df.drop(columns=[column])\n",
    "\n",
    "    # Return encoded datasets and frequency map\n",
    "    return train_df, val_df, freq_map\n",
    "\n",
    "def target_guided_ordinal_encoding(X_train, X_val, categorical_column, target_column, y_train, i):\n",
    "    # Combine X_train with y_train temporarily to calculate means\n",
    "    X_train_encoded = X_train.copy()\n",
    "    X_val_encoded = X_val.copy()\n",
    "    X_train_encoded[target_column] = y_train\n",
    "\n",
    "    means = X_train_encoded.groupby(categorical_column)[target_column].mean()\n",
    "    #print(means)\n",
    "\n",
    "    sorted_means = means.sort_values(by=target_column)\n",
    "    #print(sorted_means)\n",
    "    # if i == 1:\n",
    "    #     print(f\"Showing sorted means for {categorical_column}\")\n",
    "    #     lst_names = sorted_means.index.tolist()\n",
    "    #     lst_values = sorted_means.values.tolist()\n",
    "    #     dict_final = dict(zip(lst_names, lst_values))\n",
    "    #     print(dict_final)\n",
    "    \n",
    "    ordinal_mapping = {category: rank for rank, category in enumerate(sorted_means.index, start=1)}\n",
    "    # if i == 1:\n",
    "    #     print(f\"Showing ordinal mapping for {categorical_column}\")\n",
    "    #     print(ordinal_mapping)\n",
    "    #     print(\"--------------------------------\")\n",
    "        \n",
    "    X_train_encoded[f\"{categorical_column}_encoded\"] = X_train_encoded[categorical_column].map(ordinal_mapping)\n",
    "    X_val_encoded[f\"{categorical_column}_encoded\"] = X_val_encoded[categorical_column].map(ordinal_mapping)\n",
    "\n",
    "    #X_train_encoded = X_train_encoded.drop(columns=[categorical_column])\n",
    "    X_train_encoded = X_train_encoded.drop(columns=[target_column[0]])\n",
    "    #X_val_encoded = X_val_encoded.drop(columns=[categorical_column])\n",
    "    X_train_encoded = X_train_encoded.fillna(1)\n",
    "    X_val_encoded = X_val_encoded.fillna(1)\n",
    "\n",
    "    return X_train_encoded, X_val_encoded, ordinal_mapping\n",
    "\n",
    "# ----------------------------------- Model Optimization ----------------------------------- # \n",
    "\n",
    "\n",
    "selected_features=['Attorney/Representative',\n",
    " 'IME-4 Count',\n",
    " 'Accident Date_year',\n",
    " 'Accident Date_assembly_gap_days',\n",
    " 'C3-C2_gap_days',\n",
    " 'C2_missing',\n",
    " 'C3_missing',\n",
    " 'C3_Accident_gap_weeks',\n",
    " 'Hearing_C3 gap_months',\n",
    " 'Hearing_C2 gap_months',\n",
    " 'Days to Assembly',\n",
    " 'Days to First Hearing',\n",
    " 'Average Weekly Wage_log',\n",
    " 'Carrier Name_encoded',\n",
    " 'Carrier Type_encoded',\n",
    " 'Industry Code Description_encoded',\n",
    " 'WCIO Cause of Injury Description_encoded',\n",
    " 'WCIO Nature of Injury Description_encoded',\n",
    " 'WCIO Part Of Body Description_encoded',\n",
    " 'Carrier Name_freq',\n",
    " 'Carrier Type_freq',\n",
    " 'Industry Code Description_freq',\n",
    " 'WCIO Nature of Injury Description_freq',\n",
    " 'WCIO Part Of Body Description_freq']\n",
    "\n",
    "naive_features = [feat.replace(\"_encoded\", \"\") for feat in selected_features]\n",
    "naive_features = [feat.replace(f\"_freq\", \"\") for feat in naive_features]\n",
    "naive_features = set(naive_features)\n",
    "naive_features = list(naive_features)\n",
    "\n",
    "cat_feats = [feat for feat in naive_features if feat in cat_feats]\n",
    "\n",
    "log_reg_params = {'C': [1], \"solver\":[\"lbfgs\"], \"class_weight\":[None, \"balanced\"]}\n",
    "nb_params = {\"var_smoothing\": [1e-9, 0.1]}\n",
    "#knn_params = {'weights' : ['uniform','distance'], \"n_neighbors\":[5, 7]}\n",
    "rfc_params = {\"max_depth\": [6], \"class_weight\": [\"balanced\"]}\n",
    "#gb_params = {\"max_depth\": [3, 9], \"n_estimators\": [100, 300], \"learning_rate\": [0.1, 0.01]}\n",
    "#xgboost_params = {\"max_depth\": [6, 9], \"learning_rate\": [0.3, 0.03]}\n",
    "#hbg_params = {\"max_depth\": [6, 9], \"learning_rate\": [0.1, 0.01], \"max_iter\":[100, 200], \"class_weight\":[None, \"balanced\"]}\n",
    "catboost_params = {'iterations': [1000], 'depth':[6], 'boosting_type': ['Ordered'], \"auto_class_weights\": [\"SqrtBalanced\"], \"loss_function\": [\"MultiClassOneVsAll\"]} #\"l2_leaf_reg\":[4] # }\n",
    "nn_params = {'hidden_layer_sizes': [(25, 8)], \"learning_rate_init\": [0.01]}\n",
    "#svc_params = {\"C\": [1, 0.1], \"class_weight\": [None, \"balanced\"]}\n",
    "\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(random_state=42), log_reg_params),\n",
    "    (\"NB\", GaussianNB(), nb_params), # ele ajusta automaticamente the class priors as a parameter\n",
    "    #(\"KNNC\", KNeighborsClassifier(), knn_params),\n",
    "    (\"Random Forest\", RandomForestClassifier(random_state=42), rfc_params),\n",
    "    #(\"Gradient Boosting\", GradientBoostingClassifier(random_state=42), gb_params),\n",
    "    #(\"HGB\", HistGradientBoostingClassifier(random_state=42), hbg_params),\n",
    "    #(\"xgboost\", XGBClassifier(random_state=42, verbose=0), xgboost_params),\n",
    "    ('CatBoost', CatBoostClassifier(random_state=42, verbose=100), catboost_params),\n",
    "    (\"Neural Network\", MLPClassifier(random_state=42, verbose=10), nn_params),\n",
    "    #(\"SVC\", SVC(random_state=42), svc_params),\n",
    "]\n",
    "\n",
    "\n",
    "def optimize_thresholds(y_true, probabilities):\n",
    "    best_thresholds = []\n",
    "    for i in range(probabilities.shape[1]):  # Loop over each class\n",
    "        precision, recall, thresholds = precision_recall_curve((y_true == i).astype(int), probabilities[:, i])\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "        best_thresholds.append(thresholds[np.argmax(f1_scores)])  # Store best threshold\n",
    "    return best_thresholds\n",
    "\n",
    "def predict_with_thresholds(probabilities, thresholds):\n",
    "    weighted_probs = probabilities / np.array(thresholds)  \n",
    "    predictions = np.argmax(weighted_probs, axis=1)  \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def model_predictions_global(X_train, y_train, X_val, y_val, model, clf_name, specific_target, cv_i, params):\n",
    "    \n",
    "    train_proba = model.predict_proba(X_train)\n",
    "    val_proba = model.predict_proba(X_val)\n",
    "    \n",
    "    best_thresholds = optimize_thresholds(y_val, val_proba)\n",
    "    \n",
    "    train_predictions = predict_with_thresholds(train_proba, best_thresholds)\n",
    "    val_predictions = predict_with_thresholds(val_proba, best_thresholds)\n",
    "    \n",
    "    \n",
    "    f1_score_train = f1_score(y_train, train_predictions, average=\"macro\")\n",
    "    f1_score_val = f1_score(y_val, val_predictions, average=\"macro\")\n",
    "    \n",
    "    print(f\"{clf_name}...\")\n",
    "    print(f\"Params: {params}\")\n",
    "    print(f\"Train F1-score: {round(f1_score_train, 3)}\")\n",
    "    print(f\"Thresholds: {best_thresholds}\")\n",
    "    print(f\"Validation F1-score: {round(f1_score_val, 3)}\")\n",
    "    print(classification_report(y_val, val_predictions))\n",
    "    \n",
    "    return params, f1_score_train, f1_score_val \n",
    "\n",
    "n_splits = 3\n",
    "stratified_kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "X = df[naive_features]\n",
    "y = df[ordinal_target]\n",
    "\n",
    "X.columns\n",
    "\n",
    "y.value_counts(normalize=True)\n",
    "models_params = []\n",
    "for i, (train_index, val_index) in enumerate(stratified_kf.split(X, y), start=1):\n",
    "    print(f\"Starting CV_{i}/3 for {ordinal_target}...\")\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    #--------------- Target Ordinal Encoding \n",
    "        \n",
    "\n",
    "    print(f\"Ordinal encoding...\")\n",
    "    X_train_encoded = X_train.copy()\n",
    "    X_val_encoded = X_val.copy()\n",
    "    for cat in cat_feats:\n",
    "        X_train_encoded, X_val_encoded, ordinal_mapping = target_guided_ordinal_encoding(X_train_encoded, X_val_encoded, cat, ordinal_target, y_train, 1)\n",
    "\n",
    "    \n",
    "    columns = X_train_encoded.columns\n",
    "\n",
    "    # --------------- Target Encoding\n",
    "    \n",
    "    # num_feats = [feat for feat in X_train.columns if X_train[feat].dtype != \"object\"]\n",
    "    # cat_feats = [feat for feat in X_train.columns if X_train[feat].dtype == \"object\" and X_train[feat].nunique()]\n",
    "    \n",
    "    # te = TargetEncoder(target_type=\"multiclass\", smooth=0, random_state=42)\n",
    "    # X_train_encoded,  = te.fit_transform(X_train[cat_feats], y_train), \n",
    "    # X_val_encoded = te.transform(X_val[cat_feats])\n",
    "    # expanded_features = []\n",
    "    # for feature in te.feature_names_in_:\n",
    "    #     for i in range(1, 9):  \n",
    "    #         expanded_features.append(f\"{feature}_x_target_{i}\")\n",
    "\n",
    "    # print(expanded_features)\n",
    "    # print(f\"Total Features names   {len(expanded_features)} == transformed columns shape {X_train_encoded.shape[1]}\")\n",
    "    # X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=expanded_features, index=X_train.index)\n",
    "    # X_val_encoded_df = pd.DataFrame(X_val_encoded, columns=expanded_features, index=X_val.index)\n",
    "    \n",
    "    # X_train_encoded = pd.concat([X_train[num_feats], X_train_encoded_df], axis=1)\n",
    "    # X_val_encoded = pd.concat([X_val[num_feats], X_val_encoded_df], axis=1)\n",
    "    \n",
    "    # print(f\"Final shape of the all features: {X_train_encoded.shape[1]}\")\n",
    "    \n",
    "    # --------------- Frequency Encoding\n",
    "    print(f\"Frequency encoding...\")\n",
    "    for cat in cat_feats:\n",
    "        X_train_encoded, X_val_encoded, freq_map = frequency_encoding(X_train_encoded, X_val_encoded, cat)\n",
    "\n",
    "    \n",
    "    X_train_encoded  = X_train_encoded[selected_features]\n",
    "    X_val_encoded = X_val_encoded[selected_features]\n",
    "    \n",
    "    #print(f\"X_train columns: {X_train_encoded.columns}\")\n",
    "    \n",
    "    # --------------- Imputing missing values\n",
    "    print(f\"Impuiting missing values...\")\n",
    "    X_train_imputed, X_val_imputed = num_imputing(X_train_encoded, X_val_encoded)\n",
    "        \n",
    "    for clf_name, clf, param_grid in models:\n",
    "        # -------------- Scaling and Normalizing for some models\n",
    "        if clf_name in [\"Logistic Regression\", \"KNNC\", \"Neural Network\", \"NB\", \"SVC\"]:\n",
    "            X_train_imputed, X_val_imputed = num_scaling(X_train_imputed, X_val_imputed)\n",
    "        \n",
    "\n",
    "        # -------------- Get the possible combinations of hyperparameters for each model\n",
    "        print(f\"Testing combinations for {clf_name}...\")\n",
    "        scores_dict = {}\n",
    "\n",
    "        if param_grid:\n",
    "            keys, values = zip(*param_grid.items())\n",
    "            for combination in itertools.product(*values):\n",
    "                params = dict(zip(keys, combination))\n",
    "                if params:\n",
    "                    if clf_name == \"CatBoost\":\n",
    "                        clf = CatBoostClassifier(**params, verbose=10)\n",
    "                    else:\n",
    "                        clf.set_params(**params)\n",
    "                    clf.fit(X_train_imputed, y_train)\n",
    "                    params, train_score, val_score = model_predictions_global(X_train_imputed, y_train, X_val_imputed, y_val, clf, clf_name, ordinal_target, i, params)\n",
    "                    scores_dict[(clf_name, tuple(params.items()))] = val_score\n",
    "                models_params.append({\n",
    "                    \"clf_name\": clf_name,\n",
    "                    \"CV\":f\"CV_{i}\",\n",
    "                    \"params\": params,\n",
    "                    \"train_score\": train_score,\n",
    "                    \"validation_score\": val_score})\n",
    "                # ----------- OvRClassifier for the model\n",
    "                if clf_name == \"Random Forest\":\n",
    "                    clf.set_params(**params)\n",
    "                    clf_ovr = OneVsRestClassifier(clf)\n",
    "                    clf_ovr.fit(X_train_imputed, y_train)\n",
    "                    params, train_score, val_score = model_predictions_global(X_train_imputed, y_train, X_val_imputed, y_val, clf_ovr, f\"OVR_{clf_name}\", ordinal_target, i, params)\n",
    "                    scores_dict[(f\"OVR_{clf_name}\", tuple(params.items()))] = val_score\n",
    "                    models_params.append({\n",
    "                        \"clf_name\": f\"OVR_{clf_name}\",\n",
    "                        \"CV\":f\"CV_{i}\",\n",
    "                        \"params\": params,\n",
    "                        \"train_score\": train_score,\n",
    "                        \"validation_score\": val_score})\n",
    "        best_params, best_score = max(scores_dict.items(), key=lambda x: x[1])\n",
    "        print(f\"Best params for {clf_name}:\")\n",
    "        \n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "    print(\"--------------------------------------------------\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# ---------------- OVR voting classifier\n",
    "    \n",
    "models_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'clf_name': 'Logistic Regression',\n",
       "  'CV': 'CV_1',\n",
       "  'params': {'C': 1, 'solver': 'lbfgs', 'class_weight': None},\n",
       "  'train_score': 0.42633979167407554,\n",
       "  'validation_score': 0.41935807691859484},\n",
       " {'clf_name': 'Logistic Regression',\n",
       "  'CV': 'CV_1',\n",
       "  'params': {'C': 1, 'solver': 'lbfgs', 'class_weight': 'balanced'},\n",
       "  'train_score': 0.3799675746393409,\n",
       "  'validation_score': 0.3775670926529801},\n",
       " {'clf_name': 'NB',\n",
       "  'CV': 'CV_1',\n",
       "  'params': {'var_smoothing': 1e-09},\n",
       "  'train_score': 0.30907462369581584,\n",
       "  'validation_score': 0.3102782397360566},\n",
       " {'clf_name': 'NB',\n",
       "  'CV': 'CV_1',\n",
       "  'params': {'var_smoothing': 0.1},\n",
       "  'train_score': 0.351028190981727,\n",
       "  'validation_score': 0.3505837214917073},\n",
       " {'clf_name': 'Random Forest',\n",
       "  'CV': 'CV_1',\n",
       "  'params': {'max_depth': 6, 'class_weight': 'balanced'},\n",
       "  'train_score': 0.3687771854026147,\n",
       "  'validation_score': 0.3499940558725009},\n",
       " {'clf_name': 'OVR_Random Forest',\n",
       "  'CV': 'CV_1',\n",
       "  'params': {'max_depth': 6, 'class_weight': 'balanced'},\n",
       "  'train_score': 0.32402281906903163,\n",
       "  'validation_score': 0.30609210869809733},\n",
       " {'clf_name': 'CatBoost',\n",
       "  'CV': 'CV_1',\n",
       "  'params': {'iterations': 1000,\n",
       "   'depth': 6,\n",
       "   'boosting_type': 'Ordered',\n",
       "   'auto_class_weights': 'SqrtBalanced',\n",
       "   'loss_function': 'MultiClassOneVsAll'},\n",
       "  'train_score': 0.5585474522828763,\n",
       "  'validation_score': 0.4703015039472759},\n",
       " {'clf_name': 'Neural Network',\n",
       "  'CV': 'CV_1',\n",
       "  'params': {'hidden_layer_sizes': (25, 8), 'learning_rate_init': 0.01},\n",
       "  'train_score': 0.4577478842530862,\n",
       "  'validation_score': 0.45229168331399566},\n",
       " {'clf_name': 'Logistic Regression',\n",
       "  'CV': 'CV_2',\n",
       "  'params': {'C': 1, 'solver': 'lbfgs', 'class_weight': None},\n",
       "  'train_score': 0.43028178390907185,\n",
       "  'validation_score': 0.42819753391434634},\n",
       " {'clf_name': 'Logistic Regression',\n",
       "  'CV': 'CV_2',\n",
       "  'params': {'C': 1, 'solver': 'lbfgs', 'class_weight': 'balanced'},\n",
       "  'train_score': 0.37960003671716264,\n",
       "  'validation_score': 0.3789102380304886},\n",
       " {'clf_name': 'NB',\n",
       "  'CV': 'CV_2',\n",
       "  'params': {'var_smoothing': 1e-09},\n",
       "  'train_score': 0.30919684882025344,\n",
       "  'validation_score': 0.30747086054952644},\n",
       " {'clf_name': 'NB',\n",
       "  'CV': 'CV_2',\n",
       "  'params': {'var_smoothing': 0.1},\n",
       "  'train_score': 0.33705536402778313,\n",
       "  'validation_score': 0.3475951733773369},\n",
       " {'clf_name': 'Random Forest',\n",
       "  'CV': 'CV_2',\n",
       "  'params': {'max_depth': 6, 'class_weight': 'balanced'},\n",
       "  'train_score': 0.3617759821568079,\n",
       "  'validation_score': 0.35155257873359336},\n",
       " {'clf_name': 'OVR_Random Forest',\n",
       "  'CV': 'CV_2',\n",
       "  'params': {'max_depth': 6, 'class_weight': 'balanced'},\n",
       "  'train_score': 0.3459407674420688,\n",
       "  'validation_score': 0.32223307246263805},\n",
       " {'clf_name': 'CatBoost',\n",
       "  'CV': 'CV_2',\n",
       "  'params': {'iterations': 1000,\n",
       "   'depth': 6,\n",
       "   'boosting_type': 'Ordered',\n",
       "   'auto_class_weights': 'SqrtBalanced',\n",
       "   'loss_function': 'MultiClassOneVsAll'},\n",
       "  'train_score': 0.5206633661794804,\n",
       "  'validation_score': 0.47607401787952064},\n",
       " {'clf_name': 'Neural Network',\n",
       "  'CV': 'CV_2',\n",
       "  'params': {'hidden_layer_sizes': (25, 8), 'learning_rate_init': 0.01},\n",
       "  'train_score': 0.4430493889006046,\n",
       "  'validation_score': 0.4463632237459442},\n",
       " {'clf_name': 'Logistic Regression',\n",
       "  'CV': 'CV_3',\n",
       "  'params': {'C': 1, 'solver': 'lbfgs', 'class_weight': None},\n",
       "  'train_score': 0.41785957950249675,\n",
       "  'validation_score': 0.4195295927045751},\n",
       " {'clf_name': 'Logistic Regression',\n",
       "  'CV': 'CV_3',\n",
       "  'params': {'C': 1, 'solver': 'lbfgs', 'class_weight': 'balanced'},\n",
       "  'train_score': 0.3760102607392341,\n",
       "  'validation_score': 0.3747786798313706},\n",
       " {'clf_name': 'NB',\n",
       "  'CV': 'CV_3',\n",
       "  'params': {'var_smoothing': 1e-09},\n",
       "  'train_score': 0.26163777281049166,\n",
       "  'validation_score': 0.2601888566186618},\n",
       " {'clf_name': 'NB',\n",
       "  'CV': 'CV_3',\n",
       "  'params': {'var_smoothing': 0.1},\n",
       "  'train_score': 0.3656864634756207,\n",
       "  'validation_score': 0.35689819972972436},\n",
       " {'clf_name': 'Random Forest',\n",
       "  'CV': 'CV_3',\n",
       "  'params': {'max_depth': 6, 'class_weight': 'balanced'},\n",
       "  'train_score': 0.36750394500663564,\n",
       "  'validation_score': 0.34916183566254294},\n",
       " {'clf_name': 'OVR_Random Forest',\n",
       "  'CV': 'CV_3',\n",
       "  'params': {'max_depth': 6, 'class_weight': 'balanced'},\n",
       "  'train_score': 0.3276723198815418,\n",
       "  'validation_score': 0.3111726487897285},\n",
       " {'clf_name': 'CatBoost',\n",
       "  'CV': 'CV_3',\n",
       "  'params': {'iterations': 1000,\n",
       "   'depth': 6,\n",
       "   'boosting_type': 'Ordered',\n",
       "   'auto_class_weights': 'SqrtBalanced',\n",
       "   'loss_function': 'MultiClassOneVsAll'},\n",
       "  'train_score': 0.5604737118185696,\n",
       "  'validation_score': 0.4854034152644278},\n",
       " {'clf_name': 'Neural Network',\n",
       "  'CV': 'CV_3',\n",
       "  'params': {'hidden_layer_sizes': (25, 8), 'learning_rate_init': 0.01},\n",
       "  'train_score': 0.4514733702511858,\n",
       "  'validation_score': 0.44693730028386186}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model               | Parameters                                                                                     | Train Score | Validation Score |\n",
    "|---------------------|-----------------------------------------------------------------------------------------------|-------------|-------------------|\n",
    "| Logistic Regression | {'C': 1, 'solver': 'lbfgs', 'class_weight': Balanced''}                                             | 0.426339    | 0.419358          |\n",
    "| NB                  | {'var_smoothing': 0.1}                                                                        | 0.351028    | 0.350583          |\n",
    "| Random Forest       | {'max_depth': 6, 'class_weight': 'balanced'}                                                  | 0.3687      | 0.349994          |\n",
    "| OVR_Random Forest   | {'max_depth': 6, 'class_weight': 'balanced'}                                                  | 0.332545    | 0.313165          |\n",
    "| CatBoost            | {'iterations': 1000, 'depth': 6, 'boosting_type': 'Ordered', 'auto_class_weights': 'SqrtBalanced','loss_function': 'MultiClassOneVsAll'} | 0.560474    | 0.485403     |\n",
    "| Neural Network      | {'hidden_layer_sizes': (25, 8), 'learning_rate_init': 0.01}                                   | 0.45147    |       0.446937     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
